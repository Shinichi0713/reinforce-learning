## TD(λ)法

TD(1)はTD誤差が1step先

TD(1)ではオンライン学習可能(データ収集必要ないため)だが、データのばらつきに引っ張られて、結果の安定性がない(下手したら、最適状態を誤ってしまう)

そこでnステップ先までの目標値を推定の上、nステップ分のデータにより学習を行う方法でばらつきを防止する。

![1732336258628](image/2_td-learn/1732336258628.png)

![1732336464873](image/2_td-learn/1732336464873.png)

例えば今**t**+**1**t+1にいるとすると、その価値の差分は**t**tで観測された状態行動対の価値に、各**n**nステップTD法で**1**−**λ**,**(**1**−**λ**)**λ**,**⋯**,**(**1**−**λ**)**λ**n**−**1**,**⋯1−λ,(1−λ)λ,⋯,(1−λ)λn−1,⋯の重みで影響を与えるのだから、合計で1倍の影響を与えることになる。

##### オンライン学習

> 逐次的に環境と相互作用してデータを収集しながら学習する。学習して行動変えながら環境にフィットさせるイメージ

##### バッチ学習

> オンライン学習と相対する学習法。大量のデータを環境から取得しておき、あとで学習する方法


## 適格度トレース

メカニズムはモンテカルロ法とTD法とを統合したもの

関数近似を取り入れた状態と行動間の関係の一般化（generalization）

最後に、動的計画法とヒューリスティック探索の長所を取り入れるために環境モデルの導入です。

適格度トレース（eligibility trace）は強化学習の基本的なメカニズムの1つです。TD法を適格度トレースによって補強すると、モンテカルロ法を一方の端、1ステップTD法を他方の端とした範囲にまたがる一連の手法が作り出されます。

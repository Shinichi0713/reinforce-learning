## DDQN

Q学習までは、行動価値関数を表で表現していた。

→表のサイズは状態sを離散化した数×行動の種類で表現

表ではサイズに限りがあるため、ニューラルネットワークを使用したかったのですが、うまくいっていなかった

Q関数の表現にDeep Learningを使用したのがDQN(Deep Q-Network もしくは Deep Q-Learning Network)

DQNの出現により、より複雑なゲームや制御問題の解決が可能になり、強化学習が注目を集めました。

## 違いは？

行動価値（Q）関数が表形式からニューラルネットワークに変化したこと

Q関数の更新式自体は従来方法と変化はない

![alt text](image/5_deep-Q-Learning/1.png)

![alt text](image/5_deep-Q-Learning/2.png)

## 経験再生

Q関数のパラメータを更新するに利用する経験に、Q関数の行動の影響が出てくる→経験に偏りが生じる

→Q関数の影響をなくすために、エージェントによる観測された状態、行動、報酬を蓄積してhistory化。
→経験と呼ぶ

経験を再利用することで、一度経験を何度も利用できるし、エージェントの経験を除外した学習に利用することができる
(自動的に、方策オフ学習していることになる)

所感：オンライン学習している場合、とってくる情報には、エージェントの行動影響が出てくる→影響を除外するためには経験再生を行うことが基本となる

## 報酬のクリッピング

即時報酬を、+1、0，-1の3通り「のみ」とする。
これにより、訓練は安定しスピードが向上するとされる。
一般的には、（本来の即時報酬値が）正の場合は+1，負の場合は-1，0の場合はそのまま0。




## 実装練習

pole問題にDQNを実装してトライアルする。

![1733646739136](image/5_deep-Q-Learning/1733646739136.png)

## 背景

Q学習の課題

Q学習は遷移先の最大行動価値を用いているので、過大評価(overestimate)という問題

Q学習の更新式

![1733780322637](image/6_double_network/1733780322637.png)

過大評価の原因：赤の部分がQ学習の特徴である**最大行動価値**を利用している箇所

**たまたまある状態で高い報酬を得てしまった場合、
そこの状態を良い状態と判断してしまい、その結果がその他の状態価値にまで伝搬してしまいます。**

具体例

状態Aがスタート地点、四角がゴール地点(終点)

![1733780432743](image/6_double_network/1733780432743.png)

状態Bから左への選択肢は複数、報酬の期待値は**N**(**−**0.1**,**1**)**

状態Aからエージェントは右、左のどちらが良いかを学習

←を選択した場合の報酬期待値：(**0**−**0.1**γ**)**=**−**0.1**γ**

→本当はエージェントは状態Aでは右を選んだほうが良い


報酬テーブル

| **a**0 | **a**1 | **a**2 |
| ------------ | ------------ | ------------ |
| -1           | 1            | -0.1         |
| 0.9          | -0.5         | -0.2         |


![1733781033656](image/6_double_network/1733781033656.png)

**−**0.05**=**(**−**1**+**0.9**)**/**2**、**0.25**=**(**1**−**0.5**)**/**2**, **−**0.15**=**(**−**0.1**−**0.2**)**/**2**

この状況では**Q**(**A**,**L**e**f**t**)**>**0→左のほうが良い**


## Double Deep Q Network

**2つのQ Network** (Q Network, Target Network)を用いて、過大評価を軽減

通常のDQNのTDターゲット**T**D**Q**Nを確認

![1733781630033](image/6_double_network/1733781630033.png)

θ**−**はTarget Networkのパラメータ

**θ**はQ Networkのパラメータ

![1733781661271](image/6_double_network/1733781661271.png)

単純にTarget Networkの遷移先状態**S**t**+**1の最大行動価値**max**a**Q**(**S**t**+**1**,**a**;**θ**−**)を用いるのではなく、
Q Networkの**S**t**+**1における行動価値を最大化する行動**a**=**argmax**a**Q**(**S**t**+**1**,**a**;**θ**)**を用いてTD ターゲットの値を決める

Target Networkの最大行動価値を用いるのではなく、Q Networkを組み合わせることでQ学習の特徴（最大行動価値を使う）を維持しながら、過剰評価を軽減

## モデルベース

モデルベースによる方策の最適化はプランニング＝動的計画法により価値を推定

推定する方法は"価値反復法"と"方策勾配法"

- 価値反復法 : 価値関数が[再帰](http://d.hatena.ne.jp/keyword/%BA%C6%B5%A2)的な形で定義できることを利用して，価値関数の値が収束するまで値を更新する（価値を反復して計算する）という手法。更新するのは価値関数。
- 方策反復法 : 価値関数が最大となるように方策を更新して求めます．方策を更新すると価値関数の値も変わるので，方策の更新（方策改善）と価値関数の算出（方策評価）を交互に行って方策算出する手法。更新するのは方策。


## モデルフリー

モデルフリーな状況では、状態遷移確率が未知であるため行動価値関数（以降、簡単のため Q 関数と呼ぶ）を直接的に推定しなければならない。

価値関数の推定法としてとりえるのは、モンテカルロ法、TD 学習を Q 関数の推定に適用することができる。

つまり・・・

モデルベースとは異なり，環境（遷移関数と報酬関数）が未知の場合は，エージェントと環境の相互作用により得られたデータを用いながら価値を推定

＝モデルを明らかに用いないで価値を推定し方策を学習する方法をモデルフリー

## Q関数の制御

環境モデルが既知の場合には、可能なすべての状態と行動の組み合わせについて Q 関数が計算できるので、Q 関数について greedy な方策

![1732573203149](image/3_q-learning/1732573203149.png)

環境モデルが未知の場合、その時点までに観測された行動状態系列にしたがって Q 関数を推定するしかないので、Q 関数について greedy な方策が最適であるとは保証されない



## [モンテカルロ法](http://d.hatena.ne.jp/keyword/%A5%E2%A5%F3%A5%C6%A5%AB%A5%EB%A5%ED%CB%A1)

更新を1エピソードで行う手法を[モンテカルロ法](http://d.hatena.ne.jp/keyword/%A5%E2%A5%F3%A5%C6%A5%AB%A5%EB%A5%ED%CB%A1)といいます．更新式は次式です．

**V**π**(**s**t**)**←**V**π**(**s**t**)**+**α**(**G**t**−**V**π**(**s**t**)**)Vπ(st)←Vπ(st)+α(Gt−Vπ(st))

**G**tGt は割引報酬和です．実際に得られた即[時報](http://d.hatena.ne.jp/keyword/%BB%FE%CA%F3)酬を割り引いて足し合わせた値と推定価値の差を修正します．**α**α は[モンテカルロ法](http://d.hatena.ne.jp/keyword/%A5%E2%A5%F3%A5%C6%A5%AB%A5%EB%A5%ED%CB%A1)では状態**s**sに遷移した回数の逆数とし，平均値を出しています．
